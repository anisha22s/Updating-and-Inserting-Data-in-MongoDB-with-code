# -*- coding: utf-8 -*-
"""Untitled11.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1P23Sj1ehSaAVnuOFtfL0dnpg1QZRhvGw
"""

#Individual Project 2

#################Selenium:  The Bored Ape Yacht Club#################

#According to Wikipedia, “[The] Bored Ape Yacht Club […] is a non-fungible token (NFT) collection built on 
#the Ethereum blockchain.  The collection features profile pictures of cartoon apes that are procedurally generated 
#by an algorithm.  […]  As of 2022, [Bored Ape Yacht Club’s parent company,] Yuga Labs, is valued at US$4 billion. 
# This is due in large part to the sales of the Bored Ape Yacht Club NFT collection totaling over US$1 billion.  
#Various celebrities have purchased these non-fungible tokens, including Justin Bieber, Snoop Dogg, Gwyneth Paltrow and others.”

#(1)  (No programming yet,) go to https://opensea.io/collection/boredapeyachtclub Links to an external site. and
# select all apes with “Solid gold” fur and sort them “Price high to low” .  Use the URL for the subsequent coding.

#(2)  Using Python or Java, write code that uses Selenium to access the URL from (1), click on each of the top-8 
#most expensive Bored Apes, and store the resulting details page to disk, “bayc_[N].htm” (replace [N] with the 
#ape number).

from selenium import webdriver
from selenium.webdriver.common.keys import Keys
from selenium.webdriver.common.by import By
from selenium.webdriver.chrome.options import Options
import time
import requests
from pymongo import MongoClient
import os
from bs4 import BeautifulSoup
import json
import re



os.chdir('/Applications/') #*********please mention the working directory where your chrome driver is before running
working_direc = os.getcwd()

def Q2(working_direc):
    headers = {
        'User-Agent': 'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/109.0.0.0 Safari/537.36'
    }
    
    options = Options()
    options.add_argument("user-agent=Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/89.0.4389.82 Safari/537.36")
    
    
    #navigate to the bored ape yacht club with the specified filters
    driverpath = os.path.join(working_direc,'chromedriver')
    driver = webdriver.Chrome(executable_path=driverpath, options=options) #relative path
    driver.get('https://opensea.io/collection/boredapeyachtclub?search[sortAscending]=false&search[stringTraits][0][name]=Fur&search[stringTraits][0][values][0]=Solid%20Gold')
    time.sleep(5)

    
    #loop to click on first 8 search result and download the contents as html on local
    for i in range(1, 9):
            #product click
        products = driver.find_elements(By.CLASS_NAME, "sc-29427738-0.sc-e7851b23-1.dVNeWL.hfa-DJE.Asset--loaded") #inspect to find common element
        product = products[i-1] # Select the i-th element (0-indexed list)
        driver.execute_script("arguments[0].scrollIntoView(true);", product) #found this code on stack to avoid scroll intercept error for selenium
        product.click() #click on ith monkey
        time.sleep(5) 
                    
        html = driver.page_source
        with open(f"bayc_{i}.htm", "w") as f:
            f.write(html)
                
        driver.back() #back to main page
        time.sleep(5)
    
    time.sleep(2)
Q2(working_direc)

#################MongoDB#################
#(3)  Write code that goes through all 8 htm files downloaded in (2) and stores each ape’s name 
#(its number) and all its attributes in a document inside a MongoDB collection called “bayc”.


def Q3():
    
    client = MongoClient('mongodb://localhost:27017/') #connecting to mongodb server
    db = client['bayc'] #creating a database
    collection = db['bayc'] #collection inside database 
    
    for i in range(1, 9):
        filename = f'bayc_{i}.htm' #loop through each 8 files
        with open(filename, 'r') as f:
                    # Parse HTML with BeautifulSoup
                    soup = BeautifulSoup(f, 'html.parser')
        
                    # Find all attribute elements and extract name and value
                    name = soup.find('h1', {'class': 'sc-29427738-0 hKCSVX item--title'}).text #name of each monkey
                    attributes = {}
                    for elem in soup.find_all('div', {'class': 'Panel--isContentPadded item--properties'}): #nested loop to loop through all elements with the class Panel--isContentPadded item--properties
                        for attr_elem in elem.find_all('div', {'class': 'Property--type'}): #attribute type
                            attr = attr_elem.text.strip()
                            value = attr_elem.find_next_sibling('div', {'class': 'Property--value'}).text.strip() #attribute information
                            attributes[attr] = value
                            
                
                    # Store each ape's search resutl number, name and all attributes in a MongoDB collection
                    doc = {'i': i, 'name': name, 'attributes': attributes}
                    collection.insert_one(doc)
Q3()                   
 
#################Regular Webscraping#################
# (4) Yellow Pages uses GET requests for its search.  Using plain Python or Java (no Selenium), write a program that searches 
#on yellowpages.com for the top 30 “Pizzeria” in San Francisco (no need to verify that the shop is actually selling pizzas, 
#just search for “Pizzeria”, top 30 shops according to YP's "Default" sorting).  Save each search result page to disk, 
#“sf_pizzeria_search_page.htm”.

def Q4():

    headers = {
        'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; rv:91.0) Gecko/20100101 Firefox/91.0',
        'HTTP_CONNECTION':"keep-alive",
        'HTTP_ACCEPT':'*/*',
        'HTTP_ACCEPT_ENCODING':'gzip, deflate',
        'HTTP_HOST':'MyVeryOwnHost'}
    
    url= 'https://www.yellowpages.com/search?search_terms=pizzeria&geo_location_terms=San+Francisco%2C+CA'
    response = requests.get(url, headers = headers)
    soup = BeautifulSoup(response.text, 'lxml')
    
    # save search result page to disk
    with open('sfS_pizzeria_search_page.htm', 'w') as f:
        f.write(response.text)
    
Q4() 

#(5) Using Python or Java, write code that opens the search result page saved in 
#(4) and parses out all shop information (search rank, name, linked URL [this store’s 
#YP URL], star rating If It Exists, number of reviews IIE, TripAdvisor rating IIE, 
#number of TA reviews IIE, “$” signs IIE, years in business IIE, review IIE, and amenities
#IIE).  Please be sure to skip all “Ad” results.

# read in saved HTML file
def Q5():
    with open('sfS_pizzeria_search_page.htm', 'r') as f:
        html = f.read()
    
    
    soup = BeautifulSoup(html, 'html.parser')
    
    shop_information =[]
    
    
    for shop in soup.find_all('div', class_='v-card'):
            ad_pill = shop.find('span', class_='ad-pill') #all ads have span with class ad-pill. condition to skip this shop if it has an ad-pill span
            if ad_pill is not None:
                continue
            link = shop.select_one('a.business-name')['href']
            link = 'https://www.yellowpages.com' + link
            rank = shop.select_one('h2.n').text.strip('.')
            rank_number = rank.split('.')[0]
            
            name = shop.select_one('a.business-name').text
            
            star_rating = shop.select_one('div.result-rating')
            if star_rating:
              star_rating = star_rating['class'][1]
            else:
              star_rating = None
            
            num_reviews = shop.select_one('span.count')
            if num_reviews:
              num_reviews = num_reviews.text
            else:
              num_reviews = None
              
            tripadvratingw= shop.select_one('div[data-tripadvisor]')  #data-tripadvisor attribute
            if tripadvratingw:
                 tripadvrating = json.loads(tripadvratingw['data-tripadvisor']) #the rating and count of TA are located in a JSON string
                 Ta_rating = tripadvrating.get('rating')

            else:
                  Ta_rating = None

              
            tripadvratingw= shop.select_one('div[data-tripadvisor]')  
            if tripadvratingw:
                 tripadvrating = json.loads(tripadvratingw['data-tripadvisor']) #the rating and count of TA are located in a JSON string
                 TA_rating_count = tripadvrating.get('count')
            else:                 
                  TA_rating_count = None
            

              
            pricerange = shop.select_one('div.price-range')
            if pricerange:
                pricerange = pricerange.text
            else:
                pricerange= None
                
            yrsinbusiness = shop.select_one('div.years-in-business')
            if yrsinbusiness:
                yrsinbusiness = yrsinbusiness.text
            else:
                yrsinbusiness= None
                
            review = shop.select_one('p.body.with-avatar')
            if review:
               review = review.text
            else:
                review= None
                
            amenities = shop.select_one('div.amenities-info')
            if amenities:
               amenities = amenities.text
            else:
                amenities= None
             
            shop_information.append({
             'rank': rank_number,        
             'link': link,
             'name': name,
             'star_rating': star_rating,
             'num_reviews': num_reviews,
             'tripadvrating' :Ta_rating,
             'TA_rating_count' : TA_rating_count,
             'pricerange': pricerange,
             'yrsinbusiness' : yrsinbusiness,
             'review' : review,
             'amenities' : amenities
           
             })
    print(shop_information)
    print(len(shop_information))
Q5()


################# MongoDB#################
#(6)  Copy your code from (5).  Modify the code to create a MongoDB collection called
# “sf_pizzerias” that stores all the extracted shop information, one document for each 
#shop.

def Q6():

    # connect to MongoDB
    client = MongoClient('mongodb://localhost:27017/') 
    db = client['sf_pizzerias'] #creating a database
    pizzacollection = db['sf_pizzerias'] #collection inside database 
    
    # read in saved HTML file
    with open('sfS_pizzeria_search_page.htm', 'r') as f:
        html = f.read()
    
    
    soup = BeautifulSoup(html, 'html.parser')
    
    for shop in soup.find_all('div', class_='v-card'):
            ad_pill = shop.find('span', class_='ad-pill') #all ads have span with class ad-pill. condition to skip this shop if it has an ad-pill span
            if ad_pill is not None:
                continue
            link = shop.select_one('a.business-name')['href']
            link = 'https://www.yellowpages.com' + link
            rank = shop.select_one('h2.n').text.strip('.')
            rank_number = rank.split('.')[0]
            
            name = shop.select_one('a.business-name').text
            
            star_rating = shop.select_one('div.result-rating')
            if star_rating:
              star_rating = star_rating['class'][1]
            else:
              star_rating = None
            
            num_reviews = shop.select_one('span.count')
            if num_reviews:
              num_reviews = num_reviews.text
            else:
              num_reviews = None
            
            tripadvratingw= shop.select_one('div[data-tripadvisor]')  #data-tripadvisor attribute
            if tripadvratingw:
                 tripadvrating = json.loads(tripadvratingw['data-tripadvisor']) #the rating and count of TA are located in a JSON string
                 Ta_rating = tripadvrating.get('rating')
            
            else:
                  Ta_rating = None
            
              
            tripadvratingw= shop.select_one('div[data-tripadvisor]')  
            if tripadvratingw:
                 tripadvrating = json.loads(tripadvratingw['data-tripadvisor']) #the rating and count of TA are located in a JSON string
                 TA_rating_count = tripadvrating.get('count')
            else:                 
                  TA_rating_count = None
              
            pricerange = shop.select_one('div.price-range')
            if pricerange:
                pricerange = pricerange.text
            else:
                pricerange= None
                
            yrsinbusiness = shop.select_one('div.years-in-business')
            if yrsinbusiness:
                yrsinbusiness = yrsinbusiness.text
            else:
                yrsinbusiness= None
                
            review = shop.select_one('p.body.with-avatar')
            if review:
               review = review.text
            else:
                review= None
                
            amenities = shop.select_one('div.amenities-info')
            if amenities:
               amenities = amenities.text
            else:
                amenities= None
             
            shop_information={
             'rank': rank_number,        
             'link': link,
             'name': name,
             'star_rating': star_rating,
             'num_reviews': num_reviews,
             'tripadvrating' :Ta_rating,
             'TA_rating_count' : TA_rating_count,
             'pricerange': pricerange,
             'yrsinbusiness' : yrsinbusiness,
             'review' : review,
             'amenities' : amenities
             
           
             }
            print(shop_information)
            
            # insert the shop information into the MongoDB collection
            pizzacollection.insert_one(shop_information)
Q6()            
################# Parsing#################
#(7)  Write code that reads all URLs stored in “sf_pizzerias” and download each shop page.  
#Store the page to disk, “sf_pizzerias_[SR].htm” (replace [SR] with the search rank).


def Q7():
    with open('sfS_pizzeria_search_page.htm', 'r') as f:
        html = f.read()
    
    
    soup = BeautifulSoup(html, 'html.parser')
    
    
    #we will create a list of each search result pizzeria page url
    link = []
    
    #loop through the list and append links to the list
    for i in soup.select('a.business-name'):
            href = i['href']
            link.append('https://www.yellowpages.com' + href)
      
       
    print(link)
    len(link)
    #on further inspection, looks like this is capturing 33 links instead of 30. the extra 3 are featured pizzerias. The first and the last 2 links are features
    #lets modify that
    new_link = link[1:31]
    
    print(new_link)
    len(new_link)
    
    for i in range(0,30):
        url = new_link[i]
        response = requests.get(url)
        filename = 'SFS_pizzerias_' + str(i+1) + '.htm'
        with open(filename, 'wb') as f:
            f.write(response.content)
        time.sleep(5) #delay between requests
Q7()

#(8)  Write code that reads the 30 shop pages saved in (7) and parses each shop’s address, 
#phone number, and website.
def Q8():
    for i in range(0, 30):
        filename = 'SFS_pizzerias_' + str(i+1) + '.htm'  #loop through each 30 files
        with open(filename, 'r') as f:
           
            # Parse HTML with BeautifulSoup
            soup = BeautifulSoup(f, 'html.parser')
            name = soup.find('h1', {'class': 'business-name'}).text
            address = soup.find('span', {'class': 'address'}).text
            # Add a space before "San" in the address 
            address = re.sub(r"(San)", r" \1", address)
            phone = soup.find('p', {'class': 'phone'}).text
            website = soup.find('p', {'class': 'website'})
            if website:
                website = website.find('a')['href'] 
            else:
                website= None        
            #website = soup.find('a')['href']
            #print(f'Pizzeria {i+1}: {name} - {address}- {phone}- {website}')
            print({"rank":i+1,"name": name, "address": address, "phone": phone, "website": website})
                     
Q8()

################# API#################
#(9)  Sign up for a free account with https://positionstack.com/ Links to an external site.  
#Copy your code from (8).  Modify the code to query each shop address’ geolocation (long, lat).  
#Update each shop document on the MongoDB collection “sf_pizzerias” to contain the shop’s address, 
#phone number, website, and geolocation.




def Q9():
    client = MongoClient('mongodb://localhost:27017/')
    db = client["sf_pizzerias"]
    collection = db["sf_pizzerias"]      
    for i in range(0, 30):
        filename = 'SFS_pizzerias_' + str(i+1) + '.htm'  #loop through each 30 files
        with open(filename, 'r') as f:
           
            # Parse HTML with BeautifulSouphttps://www.google.com/url?q=https://www.google.com/chrome/?hl%3Den%26brand%3DMACD%26utm_source%3Dmacos%26utm_medium%3Dmaterial-callout%26utm_campaign%3Dgmail%26utm_content%3Dgoogle_recommends_search&source=hpp&id=19030389&ct=7&usg=AOvVaw0SwBkZsQxNRpQnrqnwIEzp&authuser=1
            url = 'http://api.positionstack.com/v1/forward'
            access_key = '49064c11a6b01718b6849e8bea4a854b'
            soup = BeautifulSoup(f, 'html.parser')
            rank = i+1
            name = soup.find('h1', {'class': 'business-name'}).text
            address = soup.find('span', {'class': 'address'}).text
            # Add a space before "San" in the address othersie the lattitude and longitude is coming wrong
            address = re.sub(r"(San)", r" \1", address)
            phone = soup.find('p', {'class': 'phone'}).text
            website = soup.find('p', {'class': 'website'})
            if website:
                website = website.find('a')['href'] 
            else:
                website= None
            params = {'access_key': access_key, 'query': address,  "country" :'US', 'region' : 'California' }
            response = requests.get(url, params=params)
            #json_response = response.json()
            #print(json.dumps(json_response, indent=2))  # print JSON response for debugging
            #latitude = response.json()['data'][0]['latitude'] 
            #longitude = response.json()['data'][0]['longitude']
            response_data = response.json()['data']
            for data in response_data:
                latitude = data['latitude']
                longitude = data['longitude']
                # do something with latitude and longitude

    
            print({"rank":str(rank),"name": name, "address": address, "latitude": latitude, "longitude": longitude})
            collection.update_one({'rank': str(rank)}, {'$set': {'address': address, 'phone': phone, 'website': website, 'latitude': latitude, 'longitude': longitude}})
            #key = {"name": name} #matching key
            #new_values = {"$set": {"address": address,"phone": phone,"website": website,"latitude": latitude,"longitude": longitude}}
            #collection.update_one(key, new_values)
Q9()